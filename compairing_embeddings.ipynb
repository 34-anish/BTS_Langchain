{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25BD_nZqRU8a"
      },
      "source": [
        "## OpenAI vs. Local Embeddings\n",
        "Performance Comparison\n",
        "- OpenAI's Embedding Model\n",
        "- InstructorEmbedding (https://huggingface.co/hkunlp/instructor-xl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjR3P6LORXQP"
      },
      "outputs": [],
      "source": [
        "!pip -q install langchain openai tiktoken chromadb pypdf sentence_transformers InstructorEmbedding faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "zhDZSwumRZhM"
      },
      "outputs": [],
      "source": [
        "from secret_key import openapi_key\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = openapi_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "Up3o9DB3Rdpm"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.document_loaders import DirectoryLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "8nDH7kaURj5I"
      },
      "outputs": [],
      "source": [
        "# InstructorEmbedding\n",
        "from InstructorEmbedding import INSTRUCTOR\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "vruCPSuxRkrL"
      },
      "outputs": [],
      "source": [
        "# OpenAI Embedding\n",
        "from langchain.embeddings import OpenAIEmbeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d26el4XSRp2N"
      },
      "source": [
        "### Load Multiple files from Directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEW8tZezRqZx",
        "outputId": "f23d93d1-562d-4cca-a25f-a86e5c6635cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# connect your Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "root_dir = \"/content/gdrive/My Drive\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "l6Z2FfiWSIw3"
      },
      "outputs": [],
      "source": [
        "# loader = TextLoader('single_text_file.txt')\n",
        "loader = DirectoryLoader(f'{root_dir}/Documents/', glob=\"./*.pdf\", loader_cls=PyPDFLoader)\n",
        "documents = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "zA1SXgawTm0x"
      },
      "outputs": [],
      "source": [
        "# documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3C9YZXJST2uh"
      },
      "source": [
        "### Divide and Conquer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "Bis-3BD4Txoo"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "                                               chunk_size=1000,\n",
        "                                               chunk_overlap=200)\n",
        "\n",
        "texts = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3625-PyT_0O",
        "outputId": "96890711-627a-4145-f444-84531d225d00"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(page_content='Gracenote.ai: Legal Generative AI for Regulatory Compliance  Jules Ioannidis, Joshua Harper, Ming Sheng Quah 1 and Dan Hunter 1, 2  1 Gracenote.ai, Melbourne Australia 2 The Dickson Poon School of Law, King’s College London, United Kingdom*   Abstract: We investigate the transformative potential of large language models (LLMs) in the legal and regulatory compliance domain by developing advanced generative AI solutions, including a horizon scanning tool, an obligations generation tool, and an LLM-based expert system. Our approach combines the LangChain framework, OpenAI’s GPT-4, text embeddings, and prompt engineering techniques to effectively reduce hallucinations and generate reliable and accurate domain-specific outputs. A human-in-the-loop control mechanism is used as a final backstop to ensure accuracy and mitigate risk. Our findings emphasise the role of LLMs as foundation engines in specialist tools and lay the groundwork for building the next generation of legal and compliance', metadata={'source': '/content/gdrive/My Drive/Documents/paper3.pdf', 'page': 0})"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "texts[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfEjgcSmUA6d",
        "outputId": "7cc7e7c0-b563-42f2-e649-cc3d6fb2f416"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "60"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwfVo4oSUHy1"
      },
      "source": [
        "### Get Embeddings for OUR Documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "qdp-IdzFUNXh"
      },
      "outputs": [],
      "source": [
        "# !pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "Kv0XDq1zUFEd"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import faiss\n",
        "from langchain.vectorstores import FAISS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "bqTbURk-UKj1"
      },
      "outputs": [],
      "source": [
        "def store_embeddings(docs, embeddings, sotre_name, path):\n",
        "\n",
        "    vectorStore = FAISS.from_documents(docs, embeddings)\n",
        "\n",
        "    with open(f\"{path}/faiss_{sotre_name}.pkl\", \"wb\") as f:\n",
        "        pickle.dump(vectorStore, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "UA3gl00wUWfk"
      },
      "outputs": [],
      "source": [
        "def load_embeddings(sotre_name, path):\n",
        "    with open(f\"{path}/faiss_{sotre_name}.pkl\", \"rb\") as f:\n",
        "        VectorStore = pickle.load(f)\n",
        "    return VectorStore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51zi5anzUbZW"
      },
      "source": [
        "### HF Instructor Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRSnS73iUX91",
        "outputId": "2d7ae013-9a00-4f16-f033-9fba2dbcc749"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "load INSTRUCTOR_Transformer\n",
            "max_seq_length  512\n"
          ]
        }
      ],
      "source": [
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "\n",
        "instructor_embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\",\n",
        "                                                      model_kwargs={\"device\": \"cuda\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "_QCyoIjUaGoM"
      },
      "outputs": [],
      "source": [
        "Embedding_store_path = f\"{root_dir}/Embedding_store\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "ZP0cn7ywKLaf"
      },
      "outputs": [],
      "source": [
        "db_instructEmbedd = FAISS.from_documents(texts, instructor_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "ia4Fhe3tbZQ0"
      },
      "outputs": [],
      "source": [
        "retriever = db_instructEmbedd.as_retriever(search_kwargs={\"k\": 3})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6_lC4EB-bj5N",
        "outputId": "d0facbed-0d4a-4b41-92f9-ab773743eb44"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'similarity'"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retriever.search_type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDU0cE8UbpBF",
        "outputId": "9b07eba4-3a04-473f-c58d-088380285382"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'k': 3}"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retriever.search_kwargs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "Fr1bteubbtoe"
      },
      "outputs": [],
      "source": [
        "docs = retriever.get_relevant_documents(\"Who is the author\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqaN6h3pfkiy",
        "outputId": "532cca5f-566c-44ec-bc33-ee68cd993da9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(page_content='interfaces, one for an author/publisher and one for the end-user/client.   For the authoring tool, upon login the author is presented with all work product that is pending from monitored feeds for that author. The author can adjust which feeds are monitored from a settings page.  The authoring environment itself has three main panes—the leftmost pane (“ASIC places interim stop orders...”) is the timeline for all content still to be published, the middle pane is the original content from the monitored source, and the rightmost pane contains the GPT-generated content. The author compares the original source with the summary to assess accuracy and quality, and they can adjust settings on the prompt and edit the summary prior to publication. This is done to reduce the hallucination/integrity issues inherent in LLMs. (A topic which we examine in more detail in section 5 below.). Refer to Figure 1 in Annexure A.  Upon publishing the content, the update is stored in a structured form and can', metadata={'source': '/content/gdrive/My Drive/Documents/paper3.pdf', 'page': 5}),\n",
              " Document(page_content='Annexure A – User interface \\nFigure 1: Authoring environment  \\nFigure 2: Client environment', metadata={'source': '/content/gdrive/My Drive/Documents/paper3.pdf', 'page': 11}),\n",
              " Document(page_content='prompts and displays this side-by-side with the material from the feed. The human author assesses the correctness of the content, can edit it as necessary, and only then publishes the content to the database—which is then used to send content externally to practice groups or client.  This type of control—often called “human-in-the-loop”—ensures that a law firm is never exposed to risk of poor-quality content going out under its name. This type of control also mitigates issues with semi-random changes in completions that can occur with some models. In essence, because a responsible human will always control the editing and dissemination of content, it doesn’t matter that a prompt to any given LLM may generate slightly different completions over time.  Privacy and confidentiality are two other hygiene concerns with the use of LLMs in legal settings. Sending personally identifying information to a public endpoint of a LLM may be a breach of various privacy laws, including the GDPR or the', metadata={'source': '/content/gdrive/My Drive/Documents/paper3.pdf', 'page': 8})]"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDlW6ZtOLjrF",
        "outputId": "247b96bb-4ab6-4547-a09a-58c8212093ff"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(page_content='interfaces, one for an author/publisher and one for the end-user/client.   For the authoring tool, upon login the author is presented with all work product that is pending from monitored feeds for that author. The author can adjust which feeds are monitored from a settings page.  The authoring environment itself has three main panes—the leftmost pane (“ASIC places interim stop orders...”) is the timeline for all content still to be published, the middle pane is the original content from the monitored source, and the rightmost pane contains the GPT-generated content. The author compares the original source with the summary to assess accuracy and quality, and they can adjust settings on the prompt and edit the summary prior to publication. This is done to reduce the hallucination/integrity issues inherent in LLMs. (A topic which we examine in more detail in section 5 below.). Refer to Figure 1 in Annexure A.  Upon publishing the content, the update is stored in a structured form and can', metadata={'source': '/content/gdrive/My Drive/Documents/paper3.pdf', 'page': 5})"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "r7hIC2WucAVf"
      },
      "outputs": [],
      "source": [
        "# create the chain to answer questions\n",
        "qa_chain_instrucEmbed = RetrievalQA.from_chain_type(llm=OpenAI(temperature=0.2, ),\n",
        "                                  chain_type=\"stuff\",\n",
        "                                  retriever=retriever,\n",
        "                                  return_source_documents=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1JGbzGNc-t4"
      },
      "source": [
        "### OpenAI's Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "2J49di9ncByE"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "GCZssU67ClpY"
      },
      "outputs": [],
      "source": [
        "embeddings = OpenAIEmbeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "cMgpDyjNcI7E"
      },
      "outputs": [],
      "source": [
        "# store_embeddings(texts,\n",
        "#                  embeddings,\n",
        "#                  sotre_name='openAIEmbeddings',\n",
        "#                  path=Embedding_store_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "FRYrIf6ZdWqF"
      },
      "outputs": [],
      "source": [
        "# db_openAIEmbedd = load_embeddings(sotre_name='openAIEmbeddings',\n",
        "#                                     path=Embedding_store_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "hlL7QEhOdlxr"
      },
      "outputs": [],
      "source": [
        "db_openAIEmbedd = FAISS.from_documents(texts, embeddings)\n",
        "retriever_openai = db_openAIEmbedd.as_retriever(search_kwargs={\"k\": 3})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "jWv6T5ZsduuS"
      },
      "outputs": [],
      "source": [
        "# create the chain to answer questions\n",
        "qa_chain_openai = RetrievalQA.from_chain_type(llm=OpenAI(temperature=0.2, ),\n",
        "                                  chain_type=\"stuff\",\n",
        "                                  retriever=retriever_openai,\n",
        "                                  return_source_documents=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKRWeYvId5po"
      },
      "source": [
        "### Testing both MODELS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "XAOlpma2d20q"
      },
      "outputs": [],
      "source": [
        "## Cite sources\n",
        "\n",
        "import textwrap\n",
        "\n",
        "def wrap_text_preserve_newlines(text, width=110):\n",
        "    # Split the input text into lines based on newline characters\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    # Wrap each line individually\n",
        "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
        "\n",
        "    # Join the wrapped lines back together using newline characters\n",
        "    wrapped_text = '\\n'.join(wrapped_lines)\n",
        "\n",
        "    return wrapped_text\n",
        "\n",
        "def process_llm_response(llm_response):\n",
        "    print(wrap_text_preserve_newlines(llm_response['result']))\n",
        "    print('\\nSources:')\n",
        "    for source in llm_response[\"source_documents\"]:\n",
        "        print(source.metadata['source'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D21M6zCUeAd7",
        "outputId": "23cb884e-9d45-4d30-8ccb-c08f7e414c01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------Instructor Embeddings------------------\n",
            "\n",
            " The authors of GPT4all Technical Report are OpenAI.\n",
            "\n",
            "Sources:\n",
            "/content/gdrive/My Drive/Documents/paper3.pdf\n",
            "/content/gdrive/My Drive/Documents/paper3.pdf\n",
            "/content/gdrive/My Drive/Documents/paper3.pdf\n"
          ]
        }
      ],
      "source": [
        "query = 'who are the authors of GPT4all technical report?'\n",
        "\n",
        "print('-------------------Instructor Embeddings------------------\\n')\n",
        "llm_response = qa_chain_instrucEmbed(query)\n",
        "process_llm_response(llm_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIknq_4MAh1v",
        "outputId": "3cf19811-249e-45a3-f334-f05e9d64f810"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------OpenAI Embeddings------------------\n",
            " I don't know.\n",
            "\n",
            "Sources:\n",
            "/content/gdrive/My Drive/Documents/paper3.pdf\n",
            "/content/gdrive/My Drive/Documents/paper3.pdf\n",
            "/content/gdrive/My Drive/Documents/paper3.pdf\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "query = 'who are the authors of GPT4all technical report?'\n",
        "\n",
        "print('-------------------OpenAI Embeddings------------------')\n",
        "llm_response = qa_chain_openai(query)\n",
        "process_llm_response(llm_response)\n",
        "print('\\n\\n\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOjsoKvCef3J",
        "outputId": "21588bc6-825e-4316-e6b5-91bb207a8a2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------Instructor Embeddings------------------\n",
            "\n",
            " GPT4All-J was trained using prompt engineering to constrain the output generated by the model.\n",
            "\n",
            "Sources:\n",
            "/content/gdrive/My Drive/Documents/paper3.pdf\n",
            "/content/gdrive/My Drive/Documents/paper3.pdf\n",
            "/content/gdrive/My Drive/Documents/paper3.pdf\n"
          ]
        }
      ],
      "source": [
        "query = 'How was the GPT4All-J model trained?'\n",
        "\n",
        "print('-------------------Instructor Embeddings------------------\\n')\n",
        "llm_response = qa_chain_instrucEmbed(query)\n",
        "process_llm_response(llm_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-g1LTybWAp90",
        "outputId": "12299303-c5b7-4438-b405-837c44ea482c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------OpenAI Embeddings------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langchain.llms.base:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-xDjxK3O2naZzvtFMMChWhYwt on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.llms.base:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-xDjxK3O2naZzvtFMMChWhYwt on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.llms.base:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-xDjxK3O2naZzvtFMMChWhYwt on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.llms.base:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-xDjxK3O2naZzvtFMMChWhYwt on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " I don't know.\n",
            "\n",
            "Sources:\n",
            "/content/gdrive/My Drive/Documents/paper3.pdf\n",
            "/content/gdrive/My Drive/Documents/paper3.pdf\n",
            "/content/gdrive/My Drive/Documents/paper3.pdf\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "query = 'How was the GPT4All-J model trained?'\n",
        "\n",
        "print('-------------------OpenAI Embeddings------------------')\n",
        "llm_response = qa_chain_openai(query)\n",
        "process_llm_response(llm_response)\n",
        "print('\\n\\n\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HGgdB0ee23d",
        "outputId": "bf06311c-7e90-4964-e2e8-f46a0c565379"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langchain.llms.base:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-xDjxK3O2naZzvtFMMChWhYwt on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------Instructor Embeddings------------------\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langchain.llms.base:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-xDjxK3O2naZzvtFMMChWhYwt on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.llms.base:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-xDjxK3O2naZzvtFMMChWhYwt on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.llms.base:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-xDjxK3O2naZzvtFMMChWhYwt on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " I don't know.\n",
            "\n",
            "Sources:\n",
            "/content/gdrive/My Drive/Documents/paper3.pdf\n",
            "/content/gdrive/My Drive/Documents/paper3.pdf\n",
            "/content/gdrive/My Drive/Documents/paper3.pdf\n"
          ]
        }
      ],
      "source": [
        "query = '\"What was the cost of training the GPT4all model?\"'\n",
        "\n",
        "print('-------------------Instructor Embeddings------------------\\n')\n",
        "llm_response = qa_chain_instrucEmbed(query)\n",
        "process_llm_response(llm_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uI9sBQrwA58N",
        "outputId": "f91aa33f-6175-422a-f905-64e766d7d7ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------OpenAI Embeddings------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langchain.llms.base:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-xDjxK3O2naZzvtFMMChWhYwt on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.llms.base:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-xDjxK3O2naZzvtFMMChWhYwt on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.llms.base:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-xDjxK3O2naZzvtFMMChWhYwt on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.llms.base:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-xDjxK3O2naZzvtFMMChWhYwt on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " I don't know.\n",
            "\n",
            "Sources:\n",
            "/content/gdrive/My Drive/Documents/paper3.pdf\n",
            "/content/gdrive/My Drive/Documents/paper3.pdf\n",
            "/content/gdrive/My Drive/Documents/paper3.pdf\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "query = '\"What was the cost of training the GPT4all model?\"'\n",
        "\n",
        "print('-------------------OpenAI Embeddings------------------')\n",
        "llm_response = qa_chain_openai(query)\n",
        "process_llm_response(llm_response)\n",
        "print('\\n\\n\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlBOGAQafEG_",
        "outputId": "91ce58ba-b4e7-432d-c011-2290081cd692"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------Instructor Embeddings------------------\n",
            "\n",
            " GPT4All-J is using an Apache 2 license.\n",
            "\n",
            "Sources:\n",
            "/content/gdrive/My Drive/Documents/2023_GPT4All-J_Technical_Report_2.pdf\n",
            "/content/gdrive/My Drive/Documents/2023_GPT4All_Technical_Report.pdf\n",
            "/content/gdrive/My Drive/Documents/2023_GPT4All-J_Technical_Report_2.pdf\n"
          ]
        }
      ],
      "source": [
        "query = \"what license is GPT4All-J using?\"\n",
        "\n",
        "# print('-------------------OpenAI Embeddings------------------')\n",
        "# llm_response = qa_chain_openai(query)\n",
        "# process_llm_response(llm_response)\n",
        "# print('\\n\\n\\n')\n",
        "print('-------------------Instructor Embeddings------------------\\n')\n",
        "llm_response = qa_chain_instrucEmbed(query)\n",
        "process_llm_response(llm_response)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
